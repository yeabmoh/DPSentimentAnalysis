Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer
did we miss it??:  {'architecture': 'standard', 'd_in': 512, 'd_sae': 32768, 'dtype': 'torch.float32', 'device': 'cpu', 'model_name': 'pythia-70m-deduped', 'hook_name': 'blocks.5.hook_resid_post', 'hook_layer': 5, 'hook_head_index': None, 'activation_fn_str': 'relu', 'activation_fn_kwargs': {}, 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': False, 'sae_lens_training_version': None, 'prepend_bos': False, 'dataset_path': 'EleutherAI/the_pile_deduplicated', 'dataset_trust_remote_code': True, 'context_size': 128, 'normalize_activations': 'none', 'neuronpedia_id': 'pythia-70m-deduped/5-res-sm'}
Loading dataset...
Processing dataset...
